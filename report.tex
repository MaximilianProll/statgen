\documentclass[a4paper,11pt]{article}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{aliascnt}
\usepackage{listings}
\usepackage{float}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage[table]{xcolor}
\usepackage{amssymb,amsthm,amsfonts, amsmath}


\begin{document}

\title{Machine learning in neonatal intensive care}

\author{Maximilian Proll \and Michele Vantini}

\maketitle

%Abstract, Introduction, [Materials], Methods, Results, Discussion and Conclusions, References. (Materials [optional] contains information about research materials, e.g. a brief description of experimental data used in your project work, if any.)

\begin{abstract}
In this work, different classification models are explored, in order to classify simulated patient data for predict in-hospital mortality. Given a simulated data set of 700 patients, composed of both basic data and time-series data, the objective is to predict if they will die in the NICU or if they will survive it.
In this project, a number of classification techniques has been assessed for classifying the basic data and the time-series data separately. In particular, for classifying time-series data, different statistics are extracted from the time-series. The obtained results are clearly suggesting that the different models are correctly learning the separation between the data points. Finally, the combination of the two groups of features allows to produce high-quality prediction by combining the results of different classifier.
\end{abstract}

\section{Introduction}
Nowadays, there are lot of concerns about newborn babies, particularly in the case of preborn infants. Indeed, in this situation babies receive particular treatments in the Neonatal Intensive Care Unit (NICU). The main reason is that there exists a correlation between premature births and developmental issues. Therefore, a number of techniques has been developed for predicting the risk of death for the monitored patients. Specifically, combining several parameters, such as heart rate, blood pressure and birth weight, it is possible to obtain a single value to use for prediction. 

The approach in this work is a little different, indeed, all the features provided in the simulated data has been used more extensively. This means that the different classifiers have been provided directly with all the features in order to further study the correlation between the possible causes (features) and the final results (the patient died or survived).

\section{Data} % aka Materials
The data that has been used in this work consists of simulated data about 700 patients. The data are divided into two data set:
\begin{itemize}
    \item Basic data: for each patient the features that are reported are:
    \begin{itemize}
        \item $ga$ - Gestational age at birth (in days)
        \item $bw$ - Birth weight (g)
    \end{itemize}
    \item Time-series data: this data set contains for each patient the time-series made of 24 hours of measurement of:
    \begin{itemize}
        \item $ABP_S$ - Arterial blood pressure, systolic
        \item $ABP_M$ - Arterial blood pressure, mean
        \item $ABP_D$ - Arterial blood pressure, diastolic
        \item $HR_{ECG}$ - Heart rate (ECG)
        \item $SpO_2$ - Oxygen saturation (pulse oximetry)
    \end{itemize}
\end{itemize}
Of all the patients, 500 represent the training set, hence, for them it is also present the label: 1 if the patient survived, 2 otherwise.

\section{Method}

In order to build a strong machine learning algorithm that can predict accurately the in-hospital mortality we used and evaluated a wide range of existing ML methods, that are used for binary classification. The ML methods we used are:

\begin{itemize}
    \item Logistic Regression
    \item Decision Trees
    \item Support Vector Machine (SVM)
    \item Gaussian Naive Bayes 
    \item Gaussian Processes
\end{itemize}

Those methods were then applied separately to the basic data set and to the extracted statistics from the time-series data and eventually to the combined information of both data sets.

The following subsections give a short explanation of the different methods used.

\subsection{Logistic Regression}

Logistic regression is a regression model where the dependent variable is categorical. In our case the dependent variable is a binary variable. But logistic regression can also be applied to cases where the dependent variable has more than two outcome categories, it is then called multinomial logistic regression.

Logistic regression uses a  predictor map $ h(.)$ with $h ( \mathbf { x } ) \in [ 0,1] $. One common choice for the predictor function $h$ is the so-called sigmoid function $ \sigma ( z ) $: 
$$ h ^ { ( \mathbf { w } ,b ) } ( \mathbf { x } ) = \sigma \left( \mathbf { w } ^ { T } \mathbf { x } + b \right) \text{ with } \sigma ( z ) : = \frac{1}{1+ \exp ( - z )}  $$

In contrast to linear regression there is no closed form solution for the cost-minimising parameters, which is why gradient descent is used to find those optimal parameters.

\subsection{Decision Trees}

Decision trees are very a very useful tool when predicting a target value as a trained decision tree is simple to understand and to interpret. Classification trees are a type of tree models where the target variable is a discrete set of values. In these tree structures, the leaves represent those class labels and branches represent conjunctions of features that lead to those class labels. 

When constructing or learning the tree structure the algorithm generally work top-down and choose those conjunctions of features at each step that best splits the set of items. Depending on the algorithm different metrics are used to choose the 'best' conjunction of features. But generally these metrics measure the homogeneity of the target variable within the subsets.

\subsection{Support Vector Machine}

Support Vector Machines (SVM) construct a hyperplane in a high- dimensional space, which then is  used for classification or regression. The premise of SVM is to select a particular hyperplane which results in a  good separation of training points. A good separation is achieved by a hyperplane that has the largest distance to the nearest training-data point of any class. In other words SVM finds those hyperplane that maximise the  functional margin. After the training a SVM are used to categorise the test data according to on which side of the hyperplane the prediction lies.

\subsection{Gaussian Naive Bayes}

In general naive Bayes classifiers are the Bayesian adaptation of a probabilistic classifiers, that is the application of Bayes' theorem with strong (\textit{naive}) independence assumptions between the features.
For continuous data one typically assumes a gaussian distribution of the data within each class.

First the data has to be segmented by the class, then the mean and variance in each class has to be computed. $ \mu _{k}$ is the mean of the values in $x$ and  $ \sigma _{k}^{2}$ denotes the variance of the values in $x$ associated with class $C_k$.
The probability of some observation value $v$ given a class $C_{k}$ follows then Normal distribution parameterised by $\mu _{k}$ and $\sigma _{k}^{2}$: 
$$  p(x=v\mid C_{k})={\frac {1}{\sqrt {2\pi \sigma _{k}^{2}}}}\,e^{-{\frac {(v-\mu _{k})^{2}}{2\sigma _{k}^{2}}}} $$

\subsection{Gaussian Processes}

A G

\section{Results}

\section{Discussion}

\section{Conclusion}

\section{References}
%\addcontentsline{toc}{section}{References}
%\bibliographystyle{unsrt}
%\bibliography{bib}


\end{document}